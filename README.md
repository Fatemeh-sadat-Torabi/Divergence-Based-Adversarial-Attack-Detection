# Divergence-Based-Adversarial-Attack-Detection
This repository contains code &amp; description files related to my master thesis. My research focuses on anomaly-based evasion techniques in low-rate DoS, distributed DoS, port scanning, masquerading, Advanced Persistent Threats (APTs), and poisoning attacks that compromise models by perturbing training data.

I evaluated the proposed method in both online and offline settings, comparing its performance with traditional machine learning algorithms such as Naive Bayes and Support Vector Machines (SVM). In the offline setting, where the entire training dataset was available upfront, models were extracted for normal and abnormal behaviors. Conversely, the online setting started with limited training data, requiring incremental updates as new data arrived in a stream. I enhanced the detection model incrementally by selectively incorporating adversarial samples that closely resembled the benign "kernel." Using a pre-trained model trained on both benign and abnormal classes with limited initial data, I adopted a test-then-train approach for newly encountered data. By measuring divergence to identify samples near the benign distribution, I ensured the integration of informative adversarial data without overwhelming the model. This process improved the model's adaptability and resilience in real-world scenarios by enabling machine learning models to adapt to evolving concepts of normal and abnormal behaviors.
Similar to the FGSM method used in [1] to perturb data, my research explored additional techniques, including applying Gaussian noise for random perturbations, optimized methods like FGSM and Jacobian-based Saliency Map Attack (JSMA) for targeted attacks, and iterative FGSM for generating stronger adversarial samples. My divergence-based approach consistently demonstrated robustness in detecting these perturbed samples, showcasing its efficacy in addressing both random and targeted adversarial attacks.



[1] Muñoz-González, L., Carnerero-Cano, J., Co, K. T., & Lupu, E. C. (2019). Challenges and advances in adversarial machine learning. Resilience and Hybrid Threats, 102-120.
